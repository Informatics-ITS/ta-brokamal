
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
INPUT_DIM = len(SRC_VOCAB)
OUTPUT_DIM = len(TGT_VOCAB)
ENC_EMB_DIM = 128
DEC_EMB_DIM = 128
HID_DIM = 256

attn = BahdanauAttention(HID_DIM, HID_DIM)
enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM)
dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, HID_DIM, attn)
model = Seq2Seq(enc, dec, SRC_VOCAB["<pad>"], device).to(device)

optimizer = optim.Adam(model.parameters())
criterion = nn.CrossEntropyLoss(ignore_index=TGT_VOCAB["<pad>"])

def levenshtein(ref, hyp):
    m, n = len(ref), len(hyp)
    dp = np.zeros((m + 1, n + 1), dtype=int)
    for i in range(m + 1):
        dp[i][0] = i
    for j in range(n + 1):
        dp[0][j] = j
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if ref[i - 1] == hyp[j - 1]:
                dp[i][j] = dp[i - 1][j - 1]
            else:
                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])
    return dp[m][n]

# Metric evaluation functions

def compute_cer(ref, hyp):
    ref = ref.replace(' ', '')
    hyp = hyp.replace(' ', '')
    if len(ref) == 0:
        return 0.0 if len(hyp) == 0 else 1.0
    return levenshtein(ref, hyp) / len(ref)

def compute_wer(ref, hyp):
    ref_words = ref.split()
    hyp_words = hyp.split()
    if len(ref_words) == 0:
        return 0.0 if len(hyp_words) == 0 else 1.0
    return levenshtein(ref_words, hyp_words) / len(ref_words)
# Use train_loader for training
best_loss = float('inf')
best_model_state = None

for epoch in range(150):
    model.train()
    total_loss = 0
    batch_losses = []
    for src, tgt, src_lens, tgt_lens in train_loader:
        src, tgt = src.to(device), tgt.to(device)
        optimizer.zero_grad()
        output = model(src, src_lens, tgt)
        output_dim = output.shape[-1]
        output = output[:,1:].reshape(-1, output_dim)
        tgt = tgt[:,1:].reshape(-1)
        loss = criterion(output, tgt)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        batch_losses.append(loss.item())
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}")
    # Save the model if it has the lowest loss so far
    if avg_loss < best_loss:
        best_loss = avg_loss
        best_model_state = model.state_dict()

# After training, load the best model
if best_model_state is not None:
    model.load_state_dict(best_model_state)
    print(f"Loaded best model with loss: {best_loss:.4f}")

Epoch 1, Loss: 6.6540
Epoch 2, Loss: 6.1047
Epoch 3, Loss: 5.4883
Epoch 4, Loss: 4.8562
Epoch 5, Loss: 4.1757
Epoch 6, Loss: 3.3963
Epoch 7, Loss: 2.6258
Epoch 8, Loss: 1.9840
Epoch 9, Loss: 1.5572
Epoch 10, Loss: 1.2240
Epoch 11, Loss: 1.0033
Epoch 12, Loss: 0.6797
Epoch 13, Loss: 0.5777
Epoch 14, Loss: 0.5274
Epoch 15, Loss: 0.4451
Epoch 16, Loss: 0.3827
Epoch 17, Loss: 0.3399
Epoch 18, Loss: 0.1924
Epoch 19, Loss: 0.1388
Epoch 20, Loss: 0.1109
Epoch 21, Loss: 0.1610
Epoch 22, Loss: 0.1839
Epoch 23, Loss: 0.0917
Epoch 24, Loss: 0.1066
Epoch 25, Loss: 0.0765
Epoch 26, Loss: 0.0669
Epoch 27, Loss: 0.0425
Epoch 28, Loss: 0.0317
Epoch 29, Loss: 0.0180
Epoch 30, Loss: 0.0122
Epoch 31, Loss: 0.0107
Epoch 32, Loss: 0.0097
Epoch 33, Loss: 0.0159
Epoch 34, Loss: 0.0137
Epoch 35, Loss: 0.0465
Epoch 36, Loss: 0.0784
Epoch 37, Loss: 0.0541
Epoch 38, Loss: 0.0823
Epoch 39, Loss: 0.3403
Epoch 40, Loss: 0.7395
Epoch 41, Loss: 0.2876
Epoch 42, Loss: 0.1776
Epoch 43, Loss: 0.0927
Epoch 44, Loss: 0.0402
Epoch 45, Loss: 0.0165
Epoch 46, Loss: 0.0088
Epoch 47, Loss: 0.0051
Epoch 48, Loss: 0.0041
Epoch 49, Loss: 0.0034
Epoch 50, Loss: 0.0030
Epoch 51, Loss: 0.0026
Epoch 52, Loss: 0.0023
Epoch 53, Loss: 0.0021
Epoch 54, Loss: 0.0019
Epoch 55, Loss: 0.0017
Epoch 56, Loss: 0.0016
Epoch 57, Loss: 0.0014
Epoch 58, Loss: 0.0013
Epoch 59, Loss: 0.0012
Epoch 60, Loss: 0.0012
Epoch 61, Loss: 0.0011
Epoch 62, Loss: 0.0010
Epoch 63, Loss: 0.0009
Epoch 64, Loss: 0.0009
Epoch 65, Loss: 0.0008
Epoch 66, Loss: 0.4051
Epoch 67, Loss: 0.3817
Epoch 68, Loss: 0.2800
Epoch 69, Loss: 0.1331
Epoch 70, Loss: 0.0511
Epoch 71, Loss: 0.0200
Epoch 72, Loss: 0.0154
Epoch 73, Loss: 0.0115
Epoch 74, Loss: 0.0046
Epoch 75, Loss: 0.0035
Epoch 76, Loss: 0.0022
Epoch 77, Loss: 0.0018
Epoch 78, Loss: 0.0016
Epoch 79, Loss: 0.0014
Epoch 80, Loss: 0.0013
Epoch 81, Loss: 0.0012
Epoch 82, Loss: 0.0011
Epoch 83, Loss: 0.0010
Epoch 84, Loss: 0.0009
Epoch 85, Loss: 0.0008
Epoch 86, Loss: 0.0008
Epoch 87, Loss: 0.0007
Epoch 88, Loss: 0.0007
Epoch 89, Loss: 0.0006
Epoch 90, Loss: 0.0006
Epoch 91, Loss: 0.0005
Epoch 92, Loss: 0.0005
Epoch 93, Loss: 0.0005
Epoch 94, Loss: 0.0005
Epoch 95, Loss: 0.0004
Epoch 96, Loss: 0.0004
Epoch 97, Loss: 0.0004
Epoch 98, Loss: 0.0004
Epoch 99, Loss: 0.0003
Epoch 100, Loss: 0.0003
Epoch 101, Loss: 0.0003
Epoch 102, Loss: 0.0003
Epoch 103, Loss: 0.0003
Epoch 104, Loss: 0.0003
Epoch 105, Loss: 0.0002
Epoch 106, Loss: 0.0002
Epoch 107, Loss: 0.0002
Epoch 108, Loss: 0.0002
Epoch 109, Loss: 0.0002
Epoch 110, Loss: 0.0002
Epoch 111, Loss: 0.0002
Epoch 112, Loss: 0.0002
Epoch 113, Loss: 0.0002
Epoch 114, Loss: 0.0002
Epoch 115, Loss: 0.0002
Epoch 116, Loss: 0.0001
Epoch 117, Loss: 0.0001
Epoch 118, Loss: 0.0001
Epoch 119, Loss: 0.0001
Epoch 120, Loss: 0.0001
Epoch 121, Loss: 0.0001
Epoch 122, Loss: 0.0001
Epoch 123, Loss: 0.0001
Epoch 124, Loss: 0.0001
Epoch 125, Loss: 0.0001
Epoch 126, Loss: 0.0001
Epoch 127, Loss: 0.0001
Epoch 128, Loss: 0.0001
Epoch 129, Loss: 0.0001
Epoch 130, Loss: 0.0001
Epoch 131, Loss: 0.0001
Epoch 132, Loss: 0.0001
Epoch 133, Loss: 0.0001
Epoch 134, Loss: 0.0001
Epoch 135, Loss: 0.0001
Epoch 136, Loss: 0.0001
Epoch 137, Loss: 0.0001
Epoch 138, Loss: 0.0001
Epoch 139, Loss: 0.0001
Epoch 140, Loss: 0.0001
Epoch 141, Loss: 0.0001
Epoch 142, Loss: 0.0001
Epoch 143, Loss: 0.0000
Epoch 144, Loss: 0.0000
Epoch 145, Loss: 0.0000
Epoch 146, Loss: 0.0000
Epoch 147, Loss: 0.0000
Epoch 148, Loss: 0.0000
Epoch 149, Loss: 0.0000
Epoch 150, Loss: 0.0000
Loaded best model with loss: 0.0000