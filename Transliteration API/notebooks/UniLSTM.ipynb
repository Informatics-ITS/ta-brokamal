{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tjQiC5UNq-3W"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import random\n",
        "\n",
        "base_source_tokens = ['source', 'nga', 'nga_ee', 'nga_uu', 'nga_ii', 'nga_ex', 'nga_oo', 'nga_eueu', 'nga_pamaeh',\n",
        "    'nga_hh', 'nga_ng', 'nga_rr',\n",
        "    'nga_ee_hh', 'nga_ee_ng', 'nga_ee_rr',\n",
        "    'nga_uu_hh', 'nga_uu_ng', 'nga_uu_rr',\n",
        "    'nga_ii_hh', 'nga_ii_ng', 'nga_ii_rr',\n",
        "    'nga_ex_hh', 'nga_ex_ng', 'nga_ex_rr',\n",
        "    'nga_oo_hh', 'nga_oo_ng', 'nga_oo_rr',\n",
        "    'nga_eueu_hh', 'nga_eueu_ng', 'nga_eueu_rr',\n",
        "    'a', 'i', 'u', 'e', 'o', 'eu', 'é', 'a_ng', 'a_hh', 'a_rr',\n",
        "          'i_ng', 'i_hh', 'i_rr',\n",
        "          'u_ng', 'u_hh', 'u_rr',\n",
        "          'e_ng', 'e_hh', 'e_rr',\n",
        "          'o_ng', 'o_hh', 'o_rr','hiji', 'dua', 'tilu', 'opat', 'lima', 'genep', 'tujuh', 'dalapan', 'salapan', 'enol', 'ka', 'na', 'pa', 'ma', 'ta', 'da', 'ba', 'ga', 'ja', 'ra', 'la', 'sa', 'ya', 'wa', 'ha', 'za', 'xa', 'fa', 'va', 'ca', 'nya', 'kha', 'sya', 'qa', 'ka_ng', 'ka_rr', 'ka_hh', 'na_ng', 'na_rr', 'na_hh', 'pa_ng', 'pa_rr', 'pa_hh', 'ma_ng', 'ma_rr', 'ma_hh', 'ta_ng', 'ta_rr', 'ta_hh', 'da_ng', 'da_rr', 'da_hh', 'ba_ng', 'ba_rr', 'ba_hh', 'ga_ng', 'ga_rr', 'ga_hh', 'ja_ng', 'ja_rr', 'ja_hh', 'ra_ng', 'ra_rr', 'ra_hh', 'la_ng', 'la_rr', 'la_hh', 'sa_ng', 'sa_rr', 'sa_hh', 'ya_ng', 'ya_rr', 'ya_hh', 'wa_ng', 'wa_rr', 'wa_hh', 'ha_ng', 'ha_rr', 'ha_hh', 'za_ng', 'za_rr', 'za_hh', 'xa_ng', 'xa_rr', 'xa_hh', 'fa_ng', 'fa_rr', 'fa_hh', 'va_ng', 'va_rr', 'va_hh', 'ca_ng', 'ca_rr', 'ca_hh', 'nya_ng', 'nya_rr', 'nya_hh', 'kha_ng', 'kha_rr', 'kha_hh', 'sya_ng', 'sya_rr', 'sya_hh', 'qa_ng', 'qa_rr', 'qa_hh', 'ka_rrr', 'ka_lll', 'ka_yyy', 'na_rrr', 'na_lll', 'na_yyy', 'pa_rrr', 'pa_lll', 'pa_yyy', 'ma_rrr', 'ma_lll', 'ma_yyy', 'ta_rrr', 'ta_lll', 'ta_yyy', 'da_rrr', 'da_lll', 'da_yyy', 'ba_rrr', 'ba_lll', 'ba_yyy', 'ga_rrr', 'ga_lll', 'ga_yyy', 'ja_rrr', 'ja_lll', 'ja_yyy', 'ra_rrr', 'ra_lll', 'ra_yyy', 'la_rrr', 'la_lll', 'la_yyy', 'sa_rrr', 'sa_lll', 'sa_yyy', 'ya_rrr', 'ya_lll', 'ya_yyy', 'wa_rrr', 'wa_lll', 'wa_yyy', 'ha_rrr', 'ha_lll', 'ha_yyy', 'za_rrr', 'za_lll', 'za_yyy', 'xa_rrr', 'xa_lll', 'xa_yyy', 'fa_rrr', 'fa_lll', 'fa_yyy', 'va_rrr', 'va_lll', 'va_yyy', 'ca_rrr', 'ca_lll', 'ca_yyy', 'qa_rrr', 'qa_lll', 'qa_yyy', 'ka_ex', 'ka_ii', 'ka_oo', 'ka_uu', 'ka_ee', 'ka_eueu', 'ka_pamaeh', 'na_ex', 'na_ii', 'na_oo', 'na_uu', 'na_ee', 'na_eueu', 'na_pamaeh', 'pa_ex', 'pa_ii', 'pa_oo', 'pa_uu', 'pa_ee', 'pa_eueu', 'pa_pamaeh', 'ma_ex', 'ma_ii', 'ma_oo', 'ma_uu', 'ma_ee', 'ma_eueu', 'ma_pamaeh', 'ta_ex', 'ta_ii', 'ta_oo', 'ta_uu', 'ta_ee', 'ta_eueu', 'ta_pamaeh', 'da_ex', 'da_ii', 'da_oo', 'da_uu', 'da_ee', 'da_eueu', 'da_pamaeh', 'ba_ex', 'ba_ii', 'ba_oo', 'ba_uu', 'ba_ee', 'ba_eueu', 'ba_pamaeh', 'ga_ex', 'ga_ii', 'ga_oo', 'ga_uu', 'ga_ee', 'ga_eueu', 'ga_pamaeh', 'ja_ex', 'ja_ii', 'ja_oo', 'ja_uu', 'ja_ee', 'ja_eueu', 'ja_pamaeh', 'ra_ex', 'ra_ii', 'ra_oo', 'ra_uu', 'ra_ee', 'ra_eueu', 'ra_pamaeh', 'la_ex', 'la_ii', 'la_oo', 'la_uu', 'la_ee', 'la_eueu', 'la_pamaeh', 'sa_ex', 'sa_ii', 'sa_oo', 'sa_uu', 'sa_ee', 'sa_eueu', 'sa_pamaeh', 'ya_ex', 'ya_ii', 'ya_oo', 'ya_uu', 'ya_ee', 'ya_eueu', 'ya_pamaeh', 'wa_ex', 'wa_ii', 'wa_oo', 'wa_uu', 'wa_ee', 'wa_eueu', 'wa_pamaeh', 'ha_ex', 'ha_ii', 'ha_oo', 'ha_uu', 'ha_ee', 'ha_eueu', 'ha_pamaeh', 'za_ex', 'za_ii', 'za_oo', 'za_uu', 'za_ee', 'za_eueu', 'za_pamaeh', 'xa_ex', 'xa_ii', 'xa_oo', 'xa_uu', 'xa_ee', 'xa_eueu', 'xa_pamaeh', 'fa_ex', 'fa_ii', 'fa_oo', 'fa_uu', 'fa_ee', 'fa_eueu', 'fa_pamaeh', 'va_ex', 'va_ii', 'va_oo', 'va_uu', 'va_ee', 'va_eueu', 'va_pamaeh', 'ca_ex', 'ca_ii', 'ca_oo', 'ca_uu', 'ca_ee', 'ca_eueu', 'ca_pamaeh', 'nya_ex', 'nya_ii', 'nya_oo', 'nya_uu', 'nya_ee', 'nya_eueu', 'nya_pamaeh', 'kha_ex', 'kha_ii', 'kha_oo', 'kha_uu', 'kha_ee', 'kha_eueu', 'kha_pamaeh', 'sya_ex', 'sya_ii', 'sya_oo', 'sya_uu', 'sya_ee', 'sya_eueu', 'sya_pamaeh', 'qa_ex', 'qa_ii', 'qa_oo', 'qa_uu', 'qa_ee', 'qa_eueu', 'qa_pamaeh', 'ka_ex_ng', 'ka_ex_rr', 'ka_ex_hh', 'ka_ii_ng', 'ka_ii_rr', 'ka_ii_hh', 'ka_oo_ng', 'ka_oo_rr', 'ka_oo_hh', 'ka_uu_ng', 'ka_uu_rr', 'ka_uu_hh', 'ka_ee_ng', 'ka_ee_rr', 'ka_ee_hh', 'ka_eueu_ng', 'ka_eueu_rr', 'ka_eueu_hh', 'na_ex_ng', 'na_ex_rr', 'na_ex_hh', 'na_ii_ng', 'na_ii_rr', 'na_ii_hh', 'na_oo_ng', 'na_oo_rr', 'na_oo_hh', 'na_uu_ng', 'na_uu_rr', 'na_uu_hh', 'na_ee_ng', 'na_ee_rr', 'na_ee_hh', 'na_eueu_ng', 'na_eueu_rr', 'na_eueu_hh', 'pa_ex_ng', 'pa_ex_rr', 'pa_ex_hh', 'pa_ii_ng', 'pa_ii_rr', 'pa_ii_hh', 'pa_oo_ng', 'pa_oo_rr', 'pa_oo_hh', 'pa_uu_ng', 'pa_uu_rr', 'pa_uu_hh', 'pa_ee_ng', 'pa_ee_rr', 'pa_ee_hh', 'pa_eueu_ng', 'pa_eueu_rr', 'pa_eueu_hh', 'ma_ex_ng', 'ma_ex_rr', 'ma_ex_hh', 'ma_ii_ng', 'ma_ii_rr', 'ma_ii_hh', 'ma_oo_ng', 'ma_oo_rr', 'ma_oo_hh', 'ma_uu_ng', 'ma_uu_rr', 'ma_uu_hh', 'ma_ee_ng', 'ma_ee_rr', 'ma_ee_hh', 'ma_eueu_ng', 'ma_eueu_rr', 'ma_eueu_hh', 'ta_ex_ng', 'ta_ex_rr', 'ta_ex_hh', 'ta_ii_ng', 'ta_ii_rr', 'ta_ii_hh', 'ta_oo_ng', 'ta_oo_rr', 'ta_oo_hh', 'ta_uu_ng', 'ta_uu_rr', 'ta_uu_hh', 'ta_ee_ng', 'ta_ee_rr', 'ta_ee_hh', 'ta_eueu_ng', 'ta_eueu_rr', 'ta_eueu_hh', 'da_ex_ng', 'da_ex_rr', 'da_ex_hh', 'da_ii_ng', 'da_ii_rr', 'da_ii_hh', 'da_oo_ng', 'da_oo_rr', 'da_oo_hh', 'da_uu_ng', 'da_uu_rr', 'da_uu_hh', 'da_ee_ng', 'da_ee_rr', 'da_ee_hh', 'da_eueu_ng', 'da_eueu_rr', 'da_eueu_hh', 'ba_ex_ng', 'ba_ex_rr', 'ba_ex_hh', 'ba_ii_ng', 'ba_ii_rr', 'ba_ii_hh', 'ba_oo_ng', 'ba_oo_rr', 'ba_oo_hh', 'ba_uu_ng', 'ba_uu_rr', 'ba_uu_hh', 'ba_ee_ng', 'ba_ee_rr', 'ba_ee_hh', 'ba_eueu_ng', 'ba_eueu_rr', 'ba_eueu_hh', 'ga_ex_ng', 'ga_ex_rr', 'ga_ex_hh', 'ga_ii_ng', 'ga_ii_rr', 'ga_ii_hh', 'ga_oo_ng', 'ga_oo_rr', 'ga_oo_hh', 'ga_uu_ng', 'ga_uu_rr', 'ga_uu_hh', 'ga_ee_ng', 'ga_ee_rr', 'ga_ee_hh', 'ga_eueu_ng', 'ga_eueu_rr', 'ga_eueu_hh', 'ja_ex_ng', 'ja_ex_rr', 'ja_ex_hh', 'ja_ii_ng', 'ja_ii_rr', 'ja_ii_hh', 'ja_oo_ng', 'ja_oo_rr', 'ja_oo_hh', 'ja_uu_ng', 'ja_uu_rr', 'ja_uu_hh', 'ja_ee_ng', 'ja_ee_rr', 'ja_ee_hh', 'ja_eueu_ng', 'ja_eueu_rr', 'ja_eueu_hh', 'ra_ex_ng', 'ra_ex_rr', 'ra_ex_hh', 'ra_ii_ng', 'ra_ii_rr', 'ra_ii_hh', 'ra_oo_ng', 'ra_oo_rr', 'ra_oo_hh', 'ra_uu_ng', 'ra_uu_rr', 'ra_uu_hh', 'ra_ee_ng', 'ra_ee_rr', 'ra_ee_hh', 'ra_eueu_ng', 'ra_eueu_rr', 'ra_eueu_hh', 'la_ex_ng', 'la_ex_rr', 'la_ex_hh', 'la_ii_ng', 'la_ii_rr', 'la_ii_hh', 'la_oo_ng', 'la_oo_rr', 'la_oo_hh', 'la_uu_ng', 'la_uu_rr', 'la_uu_hh', 'la_ee_ng', 'la_ee_rr', 'la_ee_hh', 'la_eueu_ng', 'la_eueu_rr', 'la_eueu_hh', 'sa_ex_ng', 'sa_ex_rr', 'sa_ex_hh', 'sa_ii_ng', 'sa_ii_rr', 'sa_ii_hh', 'sa_oo_ng', 'sa_oo_rr', 'sa_oo_hh', 'sa_uu_ng', 'sa_uu_rr', 'sa_uu_hh', 'sa_ee_ng', 'sa_ee_rr', 'sa_ee_hh', 'sa_eueu_ng', 'sa_eueu_rr', 'sa_eueu_hh', 'ya_ex_ng', 'ya_ex_rr', 'ya_ex_hh', 'ya_ii_ng', 'ya_ii_rr', 'ya_ii_hh', 'ya_oo_ng', 'ya_oo_rr', 'ya_oo_hh', 'ya_uu_ng', 'ya_uu_rr', 'ya_uu_hh', 'ya_ee_ng', 'ya_ee_rr', 'ya_ee_hh', 'ya_eueu_ng', 'ya_eueu_rr', 'ya_eueu_hh', 'wa_ex_ng', 'wa_ex_rr', 'wa_ex_hh', 'wa_ii_ng', 'wa_ii_rr', 'wa_ii_hh', 'wa_oo_ng', 'wa_oo_rr', 'wa_oo_hh', 'wa_uu_ng', 'wa_uu_rr', 'wa_uu_hh', 'wa_ee_ng', 'wa_ee_rr', 'wa_ee_hh', 'wa_eueu_ng', 'wa_eueu_rr', 'wa_eueu_hh', 'ha_ex_ng', 'ha_ex_rr', 'ha_ex_hh', 'ha_ii_ng', 'ha_ii_rr', 'ha_ii_hh', 'ha_oo_ng', 'ha_oo_rr', 'ha_oo_hh', 'ha_uu_ng', 'ha_uu_rr', 'ha_uu_hh', 'ha_ee_ng', 'ha_ee_rr', 'ha_ee_hh', 'ha_eueu_ng', 'ha_eueu_rr', 'ha_eueu_hh', 'za_ex_ng', 'za_ex_rr', 'za_ex_hh', 'za_ii_ng', 'za_ii_rr', 'za_ii_hh', 'za_oo_ng', 'za_oo_rr', 'za_oo_hh', 'za_uu_ng', 'za_uu_rr', 'za_uu_hh', 'za_ee_ng', 'za_ee_rr', 'za_ee_hh', 'za_eueu_ng', 'za_eueu_rr', 'za_eueu_hh', 'xa_ex_ng', 'xa_ex_rr', 'xa_ex_hh', 'xa_ii_ng', 'xa_ii_rr', 'xa_ii_hh', 'fa_ex_ng', 'fa_ex_rr', 'fa_ex_hh', 'fa_ii_ng', 'fa_ii_rr', 'fa_ii_hh', 'fa_oo_ng', 'fa_oo_rr', 'fa_oo_hh', 'fa_uu_ng', 'fa_uu_rr', 'fa_uu_hh', 'fa_ee_ng', 'fa_ee_rr', 'fa_ee_hh', 'fa_eueu_ng', 'fa_eueu_rr', 'fa_eueu_hh', 'va_ex_ng', 'va_ex_rr', 'va_ex_hh', 'va_ii_ng', 'va_ii_rr', 'va_ii_hh', 'va_oo_ng', 'va_oo_rr', 'va_oo_hh', 'va_uu_ng', 'va_uu_rr', 'va_uu_hh', 'va_ee_ng', 'va_ee_rr', 'va_ee_hh', 'va_eueu_ng', 'va_eueu_rr', 'va_eueu_hh', 'ca_ex_ng', 'ca_ex_rr', 'ca_ex_hh', 'ca_ii_ng', 'ca_ii_rr', 'ca_ii_hh', 'ca_oo_ng', 'ca_oo_rr', 'ca_oo_hh', 'ca_uu_ng', 'ca_uu_rr', 'ca_uu_hh', 'ca_ee_ng', 'ca_ee_rr', 'ca_ee_hh', 'ca_eueu_ng', 'ca_eueu_rr', 'ca_eueu_hh', 'nya_ex_ng', 'nya_ex_rr', 'nya_ex_hh', 'nya_ii_ng', 'nya_ii_rr', 'nya_ii_hh', 'nya_oo_ng', 'nya_oo_rr', 'nya_oo_hh', 'nya_uu_ng', 'nya_uu_rr', 'nya_uu_hh', 'nya_ee_ng', 'nya_ee_rr', 'nya_ee_hh', 'nya_eueu_ng', 'nya_eueu_rr', 'nya_eueu_hh', 'kha_ex_ng', 'kha_ex_rr', 'kha_ex_hh', 'kha_ee_ng', 'kha_ee_rr', 'kha_ee_hh', 'kha_eueu_ng', 'kha_eueu_rr', 'kha_eueu_hh', 'sya_ex_ng', 'sya_ex_rr', 'sya_ex_hh', 'sya_ii_ng', 'sya_ii_rr', 'sya_ii_hh', 'sya_oo_ng', 'sya_oo_rr', 'sya_oo_hh', 'sya_uu_ng', 'sya_uu_rr', 'sya_uu_hh', 'sya_ee_ng', 'sya_ee_rr', 'sya_ee_hh', 'sya_eueu_ng', 'sya_eueu_rr', 'sya_eueu_hh', 'qa_ex_ng', 'qa_ex_rr', 'qa_ex_hh', 'qa_ii_ng', 'qa_ii_rr', 'qa_ii_hh', 'qa_oo_ng', 'qa_oo_rr', 'qa_oo_hh', 'qa_uu_ng', 'qa_uu_rr', 'qa_uu_hh', 'qa_ee_ng', 'qa_ee_rr', 'qa_ee_hh', 'qa_eueu_ng', 'qa_eueu_rr', 'qa_eueu_hh', 'ka_ex_rrr', 'ka_ex_lll', 'ka_ex_yyy', 'ka_ii_rrr', 'ka_ii_lll', 'ka_ii_yyy', 'ka_oo_rrr', 'ka_oo_lll', 'ka_oo_yyy', 'ka_uu_rrr', 'ka_uu_lll', 'ka_uu_yyy', 'ka_ee_rrr', 'ka_ee_lll', 'ka_ee_yyy', 'ka_eueu_rrr', 'ka_eueu_lll', 'ka_eueu_yyy', 'na_ex_rrr', 'na_ex_lll', 'na_ex_yyy', 'na_ii_rrr', 'na_ii_lll', 'na_ii_yyy', 'na_oo_rrr', 'na_oo_lll', 'na_oo_yyy', 'na_uu_rrr', 'na_uu_lll', 'na_uu_yyy', 'na_ee_rrr', 'na_ee_lll', 'na_ee_yyy', 'na_eueu_rrr', 'na_eueu_lll', 'na_eueu_yyy', 'pa_ex_rrr', 'pa_ex_lll', 'pa_ex_yyy', 'pa_ii_rrr', 'pa_ii_lll', 'pa_ii_yyy', 'pa_oo_rrr', 'pa_oo_lll', 'pa_oo_yyy', 'pa_uu_rrr', 'pa_uu_lll', 'pa_uu_yyy', 'pa_ee_rrr', 'pa_ee_lll', 'pa_ee_yyy', 'pa_eueu_rrr', 'pa_eueu_lll', 'pa_eueu_yyy', 'ma_ex_rrr', 'ma_ex_lll', 'ma_ex_yyy', 'ma_ii_rrr', 'ma_ii_lll', 'ma_ii_yyy', 'ma_oo_rrr', 'ma_oo_lll', 'ma_oo_yyy', 'ma_uu_rrr', 'ma_uu_lll', 'ma_uu_yyy', 'ma_ee_rrr', 'ma_ee_lll', 'ma_ee_yyy', 'ma_eueu_rrr', 'ma_eueu_lll', 'ma_eueu_yyy', 'ta_ex_rrr', 'ta_ex_lll', 'ta_ex_yyy', 'ta_ii_rrr', 'ta_ii_lll', 'ta_ii_yyy', 'ta_oo_rrr', 'ta_oo_lll', 'ta_oo_yyy', 'ta_uu_rrr', 'ta_uu_lll', 'ta_uu_yyy', 'ta_ee_rrr', 'ta_ee_lll', 'ta_ee_yyy', 'ta_eueu_rrr', 'ta_eueu_lll', 'ta_eueu_yyy', 'da_ex_rrr', 'da_ex_lll', 'da_ex_yyy', 'da_ii_rrr', 'da_ii_lll', 'da_ii_yyy', 'da_oo_rrr', 'da_oo_lll', 'da_oo_yyy', 'da_uu_rrr', 'da_uu_lll', 'da_uu_yyy', 'da_ee_rrr', 'da_ee_lll', 'da_ee_yyy', 'da_eueu_rrr', 'da_eueu_lll', 'da_eueu_yyy', 'ba_ex_rrr', 'ba_ex_lll', 'ba_ex_yyy', 'ba_ii_rrr', 'ba_ii_lll', 'ba_ii_yyy', 'ba_oo_rrr', 'ba_oo_lll', 'ba_oo_yyy', 'ba_uu_rrr', 'ba_uu_lll', 'ba_uu_yyy', 'ba_ee_rrr', 'ba_ee_lll', 'ba_ee_yyy', 'ba_eueu_rrr', 'ba_eueu_lll', 'ba_eueu_yyy', 'ga_ex_rrr', 'ga_ex_lll', 'ga_ex_yyy', 'ga_ii_rrr', 'ga_ii_lll', 'ga_ii_yyy', 'ga_oo_rrr', 'ga_oo_lll', 'ga_oo_yyy', 'ga_uu_rrr', 'ga_uu_lll', 'ga_uu_yyy', 'ga_ee_rrr', 'ga_ee_lll', 'ga_ee_yyy', 'ga_eueu_rrr', 'ga_eueu_lll', 'ga_eueu_yyy', 'ja_ex_rrr', 'ja_ex_lll', 'ja_ex_yyy', 'ja_ii_rrr', 'ja_ii_lll', 'ja_ii_yyy', 'ja_oo_rrr', 'ja_oo_lll', 'ja_oo_yyy', 'ja_uu_rrr', 'ja_uu_lll', 'ja_uu_yyy', 'ja_ee_rrr', 'ja_ee_lll', 'ja_ee_yyy', 'ra_ii_rrr', 'ra_ii_lll', 'ra_ii_yyy', 'ra_oo_rrr', 'ra_oo_lll', 'ra_oo_yyy', 'ra_uu_rrr', 'ra_uu_lll', 'ra_uu_yyy', 'ra_ee_rrr', 'ra_ee_lll', 'ra_ee_yyy', 'ra_eueu_rrr', 'ra_eueu_lll', 'ra_eueu_yyy', 'la_ex_rrr', 'la_ex_lll', 'la_ex_yyy', 'la_ii_rrr', 'la_ii_lll', 'la_ii_yyy', 'la_oo_rrr', 'la_oo_lll', 'la_oo_yyy', 'la_uu_rrr', 'la_uu_lll', 'la_uu_yyy', 'la_ee_rrr', 'la_ee_lll', 'la_ee_yyy', 'sa_ex_rrr', 'sa_ex_lll', 'sa_ex_yyy', 'sa_ii_rrr', 'sa_ii_lll', 'sa_ii_yyy', 'sa_oo_rrr', 'sa_oo_lll', 'sa_oo_yyy', 'sa_uu_rrr', 'sa_uu_lll', 'sa_uu_yyy', 'sa_ee_rrr', 'sa_ee_lll', 'sa_ee_yyy', 'sa_eueu_rrr', 'sa_eueu_lll', 'sa_eueu_yyy', 'ya_ex_rrr', 'ya_ex_lll', 'ya_ex_yyy', 'ya_ii_rrr', 'ya_ii_lll', 'ya_ii_yyy', 'ya_oo_rrr', 'ya_oo_lll', 'ya_oo_yyy', 'ya_uu_rrr', 'ya_uu_lll', 'ya_uu_yyy', 'ya_ee_rrr', 'ya_ee_lll', 'ya_ee_yyy', 'ya_eueu_rrr', 'ya_eueu_lll', 'ya_eueu_yyy', 'wa_ex_rrr', 'wa_ex_lll', 'wa_ex_yyy', 'wa_ii_rrr', 'wa_ii_lll', 'wa_ii_yyy', 'wa_oo_rrr', 'wa_oo_lll', 'wa_oo_yyy', 'wa_uu_rrr', 'wa_uu_lll', 'wa_uu_yyy', 'wa_ee_rrr', 'wa_ee_lll', 'wa_ee_yyy', 'wa_eueu_rrr', 'wa_eueu_lll', 'wa_eueu_yyy', 'ha_ex_rrr', 'ha_ex_lll', 'ha_ex_yyy', 'ha_ii_rrr', 'ha_ii_lll', 'ha_ii_yyy', 'ha_oo_rrr', 'ha_oo_lll', 'ha_oo_yyy', 'ha_uu_rrr', 'ha_uu_lll', 'ha_uu_yyy', 'ha_ee_rrr', 'ha_ee_lll', 'ha_ee_yyy', 'ha_eueu_rrr', 'ha_eueu_lll', 'ha_eueu_yyy', 'fa_ex_rrr', 'fa_ex_lll', 'fa_ex_yyy', 'fa_ii_rrr', 'fa_ii_lll', 'fa_ii_yyy', 'fa_oo_rrr', 'fa_oo_lll', 'fa_oo_yyy', 'fa_uu_rrr', 'fa_uu_lll', 'fa_ee_rrr', 'fa_ee_lll', 'ca_ii_rrr', 'ca_ii_lll', 'ca_ii_yyy', 'ca_oo_rrr', 'ca_oo_lll', 'ca_oo_yyy', 'ca_uu_rrr', 'ca_uu_lll', 'ca_uu_yyy']\n",
        "base_target_tokens = [\n",
        "    'target',\n",
        "    'nga', 'nge', 'ngu', 'ngi', 'ngé', 'ngo', 'ngeu', 'ng',\n",
        "    'ngah', 'ngang', 'ngar',\n",
        "    'ngeh', 'ngeng', 'nger',\n",
        "    'nguh', 'ngung', 'ngur',\n",
        "    'ngih', 'nging', 'ngir',\n",
        "    'ngéh', 'ngéng', 'ngér',\n",
        "    'ngoh', 'ngong', 'ngor',\n",
        "    'ngeuh', 'ngeung', 'ngeur',\n",
        "    'a', 'i', 'u', 'e', 'o', 'eu', 'é', 'ang', 'ah', 'ar',\n",
        "    'ing', 'ih', 'ir',\n",
        "          'ung', 'uh', 'ur',\n",
        "          'eng', 'eh', 'er',\n",
        "          'ong', 'oh', 'or', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', 'ka', 'na', 'pa', 'ma', 'ta', 'da', 'ba', 'ga', 'ja', 'ra', 'la', 'sa', 'ya', 'wa', 'ha', 'za', 'xa', 'fa', 'va', 'ca', 'nya', 'kha', 'sya', 'qa', 'kang', 'kar', 'kah', 'nang', 'nar', 'nah', 'pang', 'par', 'pah', 'mang', 'mar', 'mah', 'tang', 'tar', 'tah', 'dang', 'dar', 'dah', 'bang', 'bar', 'bah', 'gang', 'gar', 'gah', 'jang', 'jar', 'jah', 'rang', 'rar', 'rah', 'lang', 'lar', 'lah', 'sang', 'sar', 'sah', 'yang', 'yar', 'yah', 'wang', 'war', 'wah', 'hang', 'har', 'hah', 'zang', 'zar', 'zah', 'xang', 'xar', 'xah', 'fang', 'far', 'fah', 'vang', 'var', 'vah', 'cang', 'car', 'cah', 'nyang', 'nyar', 'nyah', 'khang', 'khar', 'khah', 'syang', 'syar', 'syah', 'qang', 'qar', 'qah', 'kra', 'kla', 'kya', 'nra', 'nla', 'nya', 'pra', 'pla', 'pya', 'mra', 'mla', 'mya', 'tra', 'tla', 'tya', 'dra', 'dla', 'dya', 'bra', 'bla', 'bya', 'gra', 'gla', 'gya', 'jra', 'jla', 'jya', 'rra', 'rla', 'rya', 'lra', 'lla', 'lya', 'sra', 'sla', 'sya', 'yra', 'yla', 'yya', 'wra', 'wla', 'wya', 'hra', 'hla', 'hya', 'zra', 'zla', 'zya', 'xra', 'xla', 'xya', 'fra', 'fla', 'fya', 'vra', 'vla', 'vya', 'cra', 'cla', 'cya', 'qra', 'qla', 'qya', 'ké', 'ki', 'ko', 'ku', 'ke', 'keu', 'k', 'né', 'ni', 'no', 'nu', 'ne', 'neu', 'n', 'pé', 'pi', 'po', 'pu', 'pe', 'peu', 'p', 'mé', 'mi', 'mo', 'mu', 'me', 'meu', 'm', 'té', 'ti', 'to', 'tu', 'te', 'teu', 't', 'dé', 'di', 'do', 'du', 'de', 'deu', 'd', 'bé', 'bi', 'bo', 'bu', 'be', 'beu', 'b', 'gé', 'gi', 'go', 'gu', 'ge', 'geu', 'g', 'jé', 'ji', 'jo', 'ju', 'je', 'jeu', 'j', 'ré', 'ri', 'ro', 'ru', 're', 'reu', 'r', 'lé', 'li', 'lo', 'lu', 'le', 'leu', 'l', 'sé', 'si', 'so', 'su', 'se', 'seu', 's', 'yé', 'yi', 'yo', 'yu', 'ye', 'yeu', 'y', 'wé', 'wi', 'wo', 'wu', 'we', 'weu', 'w', 'hé', 'hi', 'ho', 'hu', 'he', 'heu', 'h', 'zé', 'zi', 'zo', 'zu', 'ze', 'zeu', 'z', 'xé', 'xi', 'xo', 'xu', 'xe', 'xeu', 'x', 'fé', 'fi', 'fo', 'fu', 'fe', 'feu', 'f', 'vé', 'vi', 'vo', 'vu', 've', 'veu', 'v', 'cé', 'ci', 'co', 'cu', 'ce', 'ceu', 'c', 'nyé', 'nyi', 'nyo', 'nyu', 'nye', 'nyeu', 'ny', 'khé', 'khi', 'kho', 'khu', 'khe', 'kheu', 'kh', 'syé', 'syi', 'syo', 'syu', 'sye', 'syeu', 'sy', 'qé', 'qi', 'qo', 'qu', 'qe', 'qeu', 'q', 'kéng', 'kér', 'kéh', 'king', 'kir', 'kih', 'kong', 'kor', 'koh', 'kung', 'kur', 'kuh', 'keng', 'ker', 'keh', 'keung', 'keur', 'keuh', 'néng', 'nér', 'néh', 'ning', 'nir', 'nih', 'nong', 'nor', 'noh', 'nung', 'nur', 'nuh', 'neng', 'ner', 'neh', 'neung', 'neur', 'neuh', 'péng', 'pér', 'péh', 'ping', 'pir', 'pih', 'pong', 'por', 'poh', 'pung', 'pur', 'puh', 'peng', 'per', 'peh', 'peung', 'peur', 'peuh', 'méng', 'mér', 'méh', 'ming', 'mir', 'mih', 'mong', 'mor', 'moh', 'mung', 'mur', 'muh', 'meng', 'mer', 'meh', 'meung', 'meur', 'meuh', 'téng', 'tér', 'téh', 'ting', 'tir', 'tih', 'tong', 'tor', 'toh', 'tung', 'tur', 'tuh', 'teng', 'ter', 'teh', 'teung', 'teur', 'teuh', 'déng', 'dér', 'déh', 'ding', 'dir', 'dih', 'dong', 'dor', 'doh', 'dung', 'dur', 'duh', 'deng', 'der', 'deh', 'deung', 'deur', 'deuh', 'béng', 'bér', 'béh', 'bing', 'bir', 'bih', 'bong', 'bor', 'boh', 'bung', 'bur', 'buh', 'beng', 'ber', 'beh', 'beung', 'beur', 'beuh', 'géng', 'gér', 'géh', 'ging', 'gir', 'gih', 'gong', 'gor', 'goh', 'gung', 'gur', 'guh', 'geng', 'ger', 'geh', 'geung', 'geur', 'geuh', 'jéng', 'jér', 'jéh', 'jing', 'jir', 'jih', 'jong', 'jor', 'joh', 'jung', 'jur', 'juh', 'jeng', 'jer', 'jeh', 'jeung', 'jeur', 'jeuh', 'réng', 'rér', 'réh', 'ring', 'rir', 'rih', 'rong', 'ror', 'roh', 'rung', 'rur', 'ruh', 'reng', 'rer', 'reh', 'reung', 'reur', 'reuh', 'léng', 'lér', 'léh', 'ling', 'lir', 'lih', 'long', 'lor', 'loh', 'lung', 'lur', 'luh', 'leng', 'ler', 'leh', 'leung', 'leur', 'leuh', 'séng', 'sér', 'séh', 'sing', 'sir', 'sih', 'song', 'sor', 'soh', 'sung', 'sur', 'suh', 'seng', 'ser', 'seh', 'seung', 'seur', 'seuh', 'yéng', 'yér', 'yéh', 'ying', 'yir', 'yih', 'yong', 'yor', 'yoh', 'yung', 'yur', 'yuh', 'yeng', 'yer', 'yeh', 'yeung', 'yeur', 'yeuh', 'wéng', 'wér', 'wéh', 'wing', 'wir', 'wih', 'wong', 'wor', 'woh', 'wung', 'wur', 'wuh', 'weng', 'wer', 'weh', 'weung', 'weur', 'weuh', 'héng', 'hér', 'héh', 'hing', 'hir', 'hih', 'hong', 'hor', 'hoh', 'hung', 'hur', 'huh', 'heng', 'her', 'heh', 'heung', 'heur', 'heuh', 'zéng', 'zér', 'zéh', 'zing', 'zir', 'zih', 'zong', 'zor', 'zoh', 'zung', 'zur', 'zuh', 'zeng', 'zer', 'zeh', 'zeung', 'zeur', 'zeuh', 'xéng', 'xér', 'xéh', 'xing', 'xir', 'xih', 'féng', 'fér', 'féh', 'fing', 'fir', 'fih', 'fong', 'for', 'foh', 'fung', 'fur', 'fuh', 'feng', 'fer', 'feh', 'feung', 'feur', 'feuh', 'véng', 'vér', 'véh', 'ving', 'vir', 'vih', 'vong', 'vor', 'voh', 'vung', 'vur', 'vuh', 'veng', 'ver', 'veh', 'veung', 'veur', 'veuh', 'céng', 'cér', 'céh', 'cing', 'cir', 'cih', 'cong', 'cor', 'coh', 'cung', 'cur', 'cuh', 'ceng', 'cer', 'ceh', 'ceung', 'ceur', 'ceuh', 'nyéng', 'nyér', 'nyéh', 'nying', 'nyir', 'nyih', 'nyong', 'nyor', 'nyoh', 'nyung', 'nyur', 'nyuh', 'nyeng', 'nyer', 'nyeh', 'nyeung', 'nyeur', 'nyeuh', 'khéng', 'khér', 'khéh', 'kheng', 'kher', 'kheh', 'kheung', 'kheur', 'kheuh', 'syéng', 'syér', 'syéh', 'sying', 'syir', 'syih', 'syong', 'syor', 'syoh', 'syung', 'syur', 'syuh', 'syeng', 'syer', 'syeh', 'syeung', 'syeur', 'syeuh', 'qéng', 'qér', 'qéh', 'qing', 'qir', 'qih', 'qong', 'qor', 'qoh', 'qung', 'qur', 'quh', 'qeng', 'qer', 'qeh', 'qeung', 'qeur', 'qeuh', 'kré', 'klé', 'kyé', 'kri', 'kli', 'kyi', 'kro', 'klo', 'kyo', 'kru', 'klu', 'kyu', 'kre', 'kle', 'kye', 'kreu', 'kleu', 'kyeu', 'nré', 'nlé', 'nyé', 'nri', 'nli', 'nyi', 'nro', 'nlo', 'nyo', 'nru', 'nlu', 'nyu', 'nre', 'nle', 'nye', 'nreu', 'nleu', 'nyeu', 'pré', 'plé', 'pyé', 'pri', 'pli', 'pyi', 'pro', 'plo', 'pyo', 'pru', 'plu', 'pyu', 'pre', 'ple', 'pye', 'preu', 'pleu', 'pyeu', 'mré', 'mlé', 'myé', 'mri', 'mli', 'myi', 'mro', 'mlo', 'myo', 'mru', 'mlu', 'myu', 'mre', 'mle', 'mye', 'mreu', 'mleu', 'myeu', 'tré', 'tlé', 'tyé', 'tri', 'tli', 'tyi', 'tro', 'tlo', 'tyo', 'tru', 'tlu', 'tyu', 'tre', 'tle', 'tye', 'treu', 'tleu', 'tyeu', 'dré', 'dlé', 'dyé', 'dri', 'dli', 'dyi', 'dro', 'dlo', 'dyo', 'dru', 'dlu', 'dyu', 'dre', 'dle', 'dye', 'dreu', 'dleu', 'dyeu', 'bré', 'blé', 'byé', 'bri', 'bli', 'byi', 'bro', 'blo', 'byo', 'bru', 'blu', 'byu', 'bre', 'ble', 'bye', 'breu', 'bleu', 'byeu', 'gré', 'glé', 'gyé', 'gri', 'gli', 'gyi', 'gro', 'glo', 'gyo', 'gru', 'glu', 'gyu', 'gre', 'gle', 'gye', 'greu', 'gleu', 'gyeu', 'jré', 'jlé', 'jyé', 'jri', 'jli', 'jyi', 'jro', 'jlo', 'jyo', 'jru', 'jlu', 'jyu', 'jre', 'jle', 'jye', 'rri', 'rli', 'ryi', 'rro', 'rlo', 'ryo', 'rru', 'rlu', 'ryu', 'rre', 'rle', 'rye', 'rreu', 'rleu', 'ryeu', 'lré', 'llé', 'lyé', 'lri', 'lli', 'lyi', 'lro', 'llo', 'lyo', 'lru', 'llu', 'lyu', 'lre', 'lle', 'lye', 'sré', 'slé', 'syé', 'sri', 'sli', 'syi', 'sro', 'slo', 'syo', 'sru', 'slu', 'syu', 'sre', 'sle', 'sye', 'sreu', 'sleu', 'syeu', 'yré', 'ylé', 'yyé', 'yri', 'yli', 'yyi', 'yro', 'ylo', 'yyo', 'yru', 'ylu', 'yyu', 'yre', 'yle', 'yye', 'yreu', 'yleu', 'yyeu', 'wré', 'wlé', 'wyé', 'wri', 'wli', 'wyi', 'wro', 'wlo', 'wyo', 'wru', 'wlu', 'wyu', 'wre', 'wle', 'wye', 'wreu', 'wleu', 'wyeu', 'hré', 'hlé', 'hyé', 'hri', 'hli', 'hyi', 'hro', 'hlo', 'hyo', 'hru', 'hlu', 'hyu', 'hre', 'hle', 'hye', 'hreu', 'hleu', 'hyeu', 'fré', 'flé', 'fyé', 'fri', 'fli', 'fyi', 'fro', 'flo', 'fyo', 'fru', 'flu', 'fre', 'fle', 'cri', 'cli', 'cyi', 'cro', 'clo', 'cyo', 'cru', 'clu', 'cyu']\n",
        "\n",
        "# dataset generation\n",
        "filename = \"dataset.csv\"\n",
        "with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"source\", \"target\"])\n",
        "\n",
        "    total_rrows = 0\n",
        "    target_rrows = 3000\n",
        "\n",
        "    # First ensure every token is used at least once\n",
        "    for src_token in base_source_tokens:\n",
        "        tgt_token = base_target_tokens[base_source_tokens.index(src_token)]\n",
        "        writer.writerow([src_token.lower(), tgt_token.lower()])\n",
        "        total_rrows += 1\n",
        "\n",
        "    # Then fill remaining rows with random combinations\n",
        "    while total_rrows < target_rrows:\n",
        "        length = random.randint(1, 25)\n",
        "        src_tokens = random.sample(base_source_tokens, k=length)\n",
        "        tgt_tokens = [\n",
        "            base_target_tokens[base_source_tokens.index(w)] if w in base_source_tokens else w\n",
        "            for w in src_tokens\n",
        "        ]\n",
        "        writer.writerow([\" \".join(src_tokens).lower(), \" \".join(tgt_tokens).lower()])\n",
        "        total_rrows += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hb5bk3-xrIfH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# 1. Data Preparation\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "\n",
        "# Split into train and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "def build_vocab(tokens):\n",
        "    vocab = {tok: i+4 for i, tok in enumerate(tokens)}\n",
        "    vocab[\"<pad>\"] = 0\n",
        "    vocab[\"<sos>\"] = 1\n",
        "    vocab[\"<eos>\"] = 2\n",
        "    vocab[\"<unk>\"] = 3\n",
        "    return vocab\n",
        "\n",
        "source_tokens = [t.lower() for t in df['source'].str.split().explode().unique()]\n",
        "target_tokens = [t.lower() for t in df['target'].str.split().explode().unique()]\n",
        "SRC_VOCAB = build_vocab(source_tokens)\n",
        "TGT_VOCAB = build_vocab(target_tokens)\n",
        "SRC_IVOCAB = {i: t for t, i in SRC_VOCAB.items()}\n",
        "TGT_IVOCAB = {i: t for t, i in TGT_VOCAB.items()}\n",
        "\n",
        "def encode(tokens, vocab):\n",
        "    return [vocab.get(tok, vocab[\"<unk>\"]) for tok in tokens]\n",
        "\n",
        "class Seq2SeqDataset(Dataset):\n",
        "    def __init__(self, df, src_vocab, tgt_vocab):\n",
        "        self.pairs = []\n",
        "        for _, row in df.iterrows():\n",
        "            src = encode(row['source'].split(), src_vocab)\n",
        "            tgt = [tgt_vocab[\"<sos>\"]] + encode(row['target'].split(), tgt_vocab) + [tgt_vocab[\"<eos>\"]]\n",
        "            self.pairs.append((src, tgt))\n",
        "    def __len__(self): return len(self.pairs)\n",
        "    def __getitem__(self, idx): return self.pairs[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    srcs, tgts = zip(*batch)\n",
        "    src_lens = [len(s) for s in srcs]\n",
        "    tgt_lens = [len(t) for t in tgts]\n",
        "    src_pad = torch.zeros(len(srcs), max(src_lens), dtype=torch.long)\n",
        "    tgt_pad = torch.zeros(len(tgts), max(tgt_lens), dtype=torch.long)\n",
        "    for i, (s, t) in enumerate(zip(srcs, tgts)):\n",
        "        src_pad[i, :len(s)] = torch.tensor(s)\n",
        "        tgt_pad[i, :len(t)] = torch.tensor(t)\n",
        "    return src_pad, tgt_pad, src_lens, tgt_lens\n",
        "\n",
        "# Use train_df and test_df to create datasets and loaders\n",
        "train_dataset = Seq2SeqDataset(train_df, SRC_VOCAB, TGT_VOCAB)\n",
        "test_dataset = Seq2SeqDataset(test_df, SRC_VOCAB, TGT_VOCAB)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSwTApC3rPzo"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 2. Model Definition\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, n_layers=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=SRC_VOCAB[\"<pad>\"])\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, bidirectional=False, batch_first=True)\n",
        "    def forward(self, src, src_lens):\n",
        "        emb = self.embedding(src)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(emb, src_lens, batch_first=True, enforce_sorted=False)\n",
        "        outputs, (h, c) = self.lstm(packed)\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "        return outputs, (h, c)\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(enc_hid_dim + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Parameter(torch.rand(dec_hid_dim))\n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        src_len = encoder_outputs.size(1)\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        energy = energy @ self.v\n",
        "        energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        return F.softmax(energy, dim=1)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, enc_hid_dim, dec_hid_dim, attention, n_layers=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=TGT_VOCAB[\"<pad>\"])\n",
        "        self.lstm = nn.LSTM(emb_dim + enc_hid_dim, dec_hid_dim, n_layers, batch_first=True)\n",
        "        self.fc_out = nn.Linear(dec_hid_dim + enc_hid_dim + emb_dim, vocab_size)\n",
        "        self.attention = attention\n",
        "    def forward(self, input, hidden, cell, encoder_outputs, mask):\n",
        "        input = input.unsqueeze(1)\n",
        "        emb = self.embedding(input)\n",
        "        attn_weights = self.attention(hidden[-1], encoder_outputs, mask)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
        "        lstm_input = torch.cat((emb, attn_applied), dim=2)\n",
        "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
        "        output = output.squeeze(1)\n",
        "        attn_applied = attn_applied.squeeze(1)\n",
        "        emb = emb.squeeze(1)\n",
        "        pred = self.fc_out(torch.cat((output, attn_applied, emb), dim=1))\n",
        "        return pred, hidden, cell, attn_weights\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.device = device\n",
        "    def create_mask(self, src):\n",
        "        return (src != self.src_pad_idx)\n",
        "    def forward(self, src, src_lens, tgt, teacher_forcing_ratio=0.5):\n",
        "        batch_size, tgt_len = tgt.shape\n",
        "        outputs = torch.zeros(batch_size, tgt_len, self.decoder.embedding.num_embeddings).to(self.device)\n",
        "        encoder_outputs, (h, c) = self.encoder(src, src_lens)\n",
        "        h_dec = h  # [num_layers, batch, hid_dim]\n",
        "        c_dec = c\n",
        "        input = tgt[:,0]\n",
        "        mask = self.create_mask(src)\n",
        "        for t in range(1, tgt_len):\n",
        "            output, h_dec, c_dec, _ = self.decoder(input, h_dec, c_dec, encoder_outputs, mask)\n",
        "            outputs[:,t] = output\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = tgt[:,t] if teacher_force else top1\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrbOCjeZrTts",
        "outputId": "b4413595-ac24-4586-d2c2-7fc8466b08f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 6.5907\n",
            "Epoch 2, Loss: 6.0480\n",
            "Epoch 3, Loss: 5.4318\n",
            "Epoch 4, Loss: 4.6996\n",
            "Epoch 5, Loss: 3.9214\n",
            "Epoch 6, Loss: 3.1052\n",
            "Epoch 7, Loss: 2.3424\n",
            "Epoch 8, Loss: 1.6936\n",
            "Epoch 9, Loss: 1.2260\n",
            "Epoch 10, Loss: 0.8133\n",
            "Epoch 11, Loss: 0.5500\n",
            "Epoch 12, Loss: 0.3766\n",
            "Epoch 13, Loss: 0.2632\n",
            "Epoch 14, Loss: 0.1795\n",
            "Epoch 15, Loss: 0.1307\n",
            "Epoch 16, Loss: 0.1078\n",
            "Epoch 17, Loss: 0.0963\n",
            "Epoch 18, Loss: 0.0642\n",
            "Epoch 19, Loss: 0.0532\n",
            "Epoch 20, Loss: 0.0626\n",
            "Epoch 21, Loss: 0.0574\n",
            "Epoch 22, Loss: 0.0644\n",
            "Epoch 23, Loss: 0.0576\n",
            "Epoch 24, Loss: 0.0565\n",
            "Epoch 25, Loss: 0.0490\n",
            "Epoch 26, Loss: 0.0659\n",
            "Epoch 27, Loss: 0.0577\n",
            "Epoch 28, Loss: 0.0409\n",
            "Epoch 29, Loss: 0.0532\n",
            "Epoch 30, Loss: 0.0634\n",
            "Epoch 31, Loss: 0.0640\n",
            "Epoch 32, Loss: 0.0394\n",
            "Epoch 33, Loss: 0.0257\n",
            "Epoch 34, Loss: 0.0184\n",
            "Epoch 35, Loss: 0.0182\n",
            "Epoch 36, Loss: 0.0138\n",
            "Epoch 37, Loss: 0.0098\n",
            "Epoch 38, Loss: 0.0097\n",
            "Epoch 39, Loss: 0.0077\n",
            "Epoch 40, Loss: 0.0053\n",
            "Epoch 41, Loss: 0.0032\n",
            "Epoch 42, Loss: 0.0027\n",
            "Epoch 43, Loss: 0.0024\n",
            "Epoch 44, Loss: 0.0022\n",
            "Epoch 45, Loss: 0.0021\n",
            "Epoch 46, Loss: 0.0019\n",
            "Epoch 47, Loss: 0.0018\n",
            "Epoch 48, Loss: 0.0017\n",
            "Epoch 49, Loss: 0.0016\n",
            "Epoch 50, Loss: 0.0015\n",
            "Epoch 51, Loss: 0.0014\n",
            "Epoch 52, Loss: 0.0013\n",
            "Epoch 53, Loss: 0.0013\n",
            "Epoch 54, Loss: 0.0012\n",
            "Epoch 55, Loss: 0.0011\n",
            "Epoch 56, Loss: 0.0011\n",
            "Epoch 57, Loss: 0.0010\n",
            "Epoch 58, Loss: 0.0010\n",
            "Epoch 59, Loss: 0.0009\n",
            "Epoch 60, Loss: 0.0009\n",
            "Epoch 61, Loss: 0.0008\n",
            "Epoch 62, Loss: 0.0008\n",
            "Epoch 63, Loss: 0.0008\n",
            "Epoch 64, Loss: 0.0007\n",
            "Epoch 65, Loss: 0.0007\n",
            "Epoch 66, Loss: 0.0007\n",
            "Epoch 67, Loss: 0.0006\n",
            "Epoch 68, Loss: 0.0006\n",
            "Epoch 69, Loss: 0.0006\n",
            "Epoch 70, Loss: 0.0006\n",
            "Epoch 71, Loss: 0.0005\n",
            "Epoch 72, Loss: 0.0005\n",
            "Epoch 73, Loss: 0.0005\n",
            "Epoch 74, Loss: 0.0005\n",
            "Epoch 75, Loss: 0.0004\n",
            "Epoch 76, Loss: 0.0004\n",
            "Epoch 77, Loss: 0.0004\n",
            "Epoch 78, Loss: 0.0004\n",
            "Epoch 79, Loss: 0.0004\n",
            "Epoch 80, Loss: 0.0004\n",
            "Epoch 81, Loss: 0.0003\n",
            "Epoch 82, Loss: 0.0003\n",
            "Epoch 83, Loss: 0.0003\n",
            "Epoch 84, Loss: 0.0003\n",
            "Epoch 85, Loss: 0.0003\n",
            "Epoch 86, Loss: 0.0003\n",
            "Epoch 87, Loss: 0.0003\n",
            "Epoch 88, Loss: 0.0003\n",
            "Epoch 89, Loss: 0.0003\n",
            "Epoch 90, Loss: 0.0002\n",
            "Epoch 91, Loss: 0.0002\n",
            "Epoch 92, Loss: 0.0002\n",
            "Epoch 93, Loss: 0.0002\n",
            "Epoch 94, Loss: 0.0002\n",
            "Epoch 95, Loss: 0.0002\n",
            "Epoch 96, Loss: 0.0002\n",
            "Epoch 97, Loss: 0.0002\n",
            "Epoch 98, Loss: 0.0002\n",
            "Epoch 99, Loss: 0.0002\n",
            "Epoch 100, Loss: 0.0002\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "INPUT_DIM = len(SRC_VOCAB)\n",
        "OUTPUT_DIM = len(TGT_VOCAB)\n",
        "ENC_EMB_DIM = 128\n",
        "DEC_EMB_DIM = 128\n",
        "HID_DIM = 256\n",
        "\n",
        "attn = BahdanauAttention(HID_DIM, HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, HID_DIM, attn)\n",
        "model = Seq2Seq(enc, dec, SRC_VOCAB[\"<pad>\"], device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TGT_VOCAB[\"<pad>\"])\n",
        "\n",
        "# Metric evaluation functions\n",
        "def levenshtein(ref, hyp):\n",
        "    m, n = len(ref), len(hyp)\n",
        "    dp = np.zeros((m + 1, n + 1), dtype=int)\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            if ref[i - 1] == hyp[j - 1]:\n",
        "                dp[i][j] = dp[i - 1][j - 1]\n",
        "            else:\n",
        "                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n",
        "    return dp[m][n]\n",
        "\n",
        "def compute_cer(ref, hyp):\n",
        "    ref = ref.replace(' ', '')\n",
        "    hyp = hyp.replace(' ', '')\n",
        "    if len(ref) == 0:\n",
        "        return 0.0 if len(hyp) == 0 else 1.0\n",
        "    return levenshtein(ref, hyp) / len(ref)\n",
        "\n",
        "def compute_wer(ref, hyp):\n",
        "    ref_words = ref.split()\n",
        "    hyp_words = hyp.split()\n",
        "    if len(ref_words) == 0:\n",
        "        return 0.0 if len(hyp_words) == 0 else 1.0\n",
        "    return levenshtein(ref_words, hyp_words) / len(ref_words)\n",
        "\n",
        "# Use train_loader for training\n",
        "best_loss = float('inf')\n",
        "best_model_state = None\n",
        "\n",
        "for epoch in range(150):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    batch_losses = []\n",
        "    for src, tgt, src_lens, tgt_lens in train_loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, src_lens, tgt)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:,1:].reshape(-1, output_dim)\n",
        "        tgt = tgt[:,1:].reshape(-1)\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        batch_losses.append(loss.item())\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
        "    # Save the model if it has the lowest loss so far\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        best_model_state = model.state_dict()\n",
        "\n",
        "# After training, load the best model\n",
        "if best_model_state is not None:\n",
        "    model.load_state_dict(best_model_state)\n",
        "    print(f\"Loaded best model with loss: {best_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "cAc8Nm_AsTq6",
        "outputId": "397b44ab-6276-4097-c880-c4f2a30c2d99"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-63caad6a2136>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Example usage:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ka na pa\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "def translate(model, src_sentence, max_len=30):\n",
        "    model.eval()\n",
        "    src_tokens = encode(src_sentence.lower().split(), SRC_VOCAB)\n",
        "    src_tensor = torch.tensor(src_tokens).unsqueeze(0).to(device)\n",
        "    src_lens = [len(src_tokens)]\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, (h, c) = model.encoder(src_tensor, src_lens)\n",
        "        h_dec = h\n",
        "        c_dec = c\n",
        "        input = torch.tensor([TGT_VOCAB[\"<sos>\"]]).to(device)\n",
        "        mask = model.create_mask(src_tensor)\n",
        "        result = []\n",
        "        for _ in range(max_len):\n",
        "            output, h_dec, c_dec, _ = model.decoder(input, h_dec, c_dec, encoder_outputs, mask)\n",
        "            top1 = output.argmax(1)\n",
        "            if top1.item() == TGT_VOCAB[\"<eos>\"]:\n",
        "                break\n",
        "            result.append(TGT_IVOCAB[top1.item()])\n",
        "            input = top1\n",
        "    return \" \".join(result)\n",
        "\n",
        "# Example usage:\n",
        "print(translate(model, \"ka na pa\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "b3xn3AXdsWCs",
        "outputId": "02f12b45-8a5e-4c11-c7a1-c558083ed496"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bd6aea24a601>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Metric evaluation on the test set (unseen data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtotal_cer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtotal_wer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "    # Metric evaluation on the test set (unseen data)\n",
        "    model.eval()\n",
        "    total_cer = 0\n",
        "    total_wer = 0\n",
        "    n_samples = 0\n",
        "    with torch.no_grad():\n",
        "        example_printed = 0\n",
        "        for src, tgt, src_lens, tgt_lens in test_loader:\n",
        "            src = src.to(device)\n",
        "            for i in range(src.size(0)):\n",
        "                src_sentence = ' '.join([SRC_IVOCAB[idx.item()] for idx in src[i] if idx.item() != SRC_VOCAB[\"<pad>\"]])\n",
        "                tgt_sentence = ' '.join([TGT_IVOCAB[idx.item()] for idx in tgt[i] if idx.item() not in [TGT_VOCAB[\"<pad>\"], TGT_VOCAB[\"<sos>\"], TGT_VOCAB[\"<eos>\"]]])\n",
        "                pred_sentence = translate(model, src_sentence)\n",
        "                cer = compute_cer(tgt_sentence, pred_sentence)\n",
        "                wer = compute_wer(tgt_sentence, pred_sentence)\n",
        "                total_cer += cer\n",
        "                total_wer += wer\n",
        "                n_samples += 1\n",
        "                # Print a few examples\n",
        "                if example_printed < 100:\n",
        "                    print(f\"Example {example_printed+1}:\")\n",
        "                    print(f\"  Source:    {src_sentence}\")\n",
        "                    print(f\"  Target:    {tgt_sentence}\")\n",
        "                    print(f\"  Predicted: {pred_sentence}\")\n",
        "                    print(f\"  CER: {cer:.2%}, WER: {wer:.2%}\\n\")\n",
        "                    example_printed += 1\n",
        "    print(f\"CER: {total_cer/n_samples:.2%}, WER: {total_wer/n_samples:.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd5vtMDzsW6k",
        "outputId": "03c37c3c-7f06-4139-a4e6-e516d2b7032e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "ka\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# =========================\n",
        "# 9. Try Inference\n",
        "# =========================\n",
        "def translate(text):\n",
        "    seq = src_tokenizer.texts_to_sequences([text])\n",
        "    seq = pad_sequences(seq, maxlen=max_encoder_seq_length, padding='post')\n",
        "    return decode_sequence(seq)\n",
        "\n",
        "# Example:\n",
        "print(translate(\"ka ma l\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
